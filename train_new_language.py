# -*- coding: utf-8 -*-
"""Tacotron2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZRkyDpvoMfhP5gVTloDKXY580a33kJO5

# Import Libraries
"""

!pip install SpeechRecognition

from google.colab import files

!pip install pydub 
#Install pydub

# importing libraries
import speech_recognition as sr
import os, pathlib, json
from pydub import AudioSegment
import pydub
from pydub.silence import split_on_silence
import IPython
from IPython.display import Audio, clear_output, display

!pip install scipy

!pip install librosa

#import matplotlib.pyplot as plt
from scipy import signal
from scipy.io import wavfile
import os 
from os.path import exists, join, expanduser
import random
import librosa  
import soundfile as sf
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import os
from os.path import exists, join, basename, splitext

git_repo_url = 'https://github.com/NVIDIA/tacotron2.git'
project_name = splitext(basename(git_repo_url))[0]
if not exists(project_name):
  # clone and install
  !git clone -q --recursive {git_repo_url}
  !cd {project_name}/waveglow && git checkout 9168aea
  !pip install -q librosa unidecode
  
import sys
sys.path.append(join(project_name, 'waveglow/'))
sys.path.append(project_name)
import time

import matplotlib
import matplotlib.pylab as plt
plt.rcParams["axes.grid"] = False

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import os
if os.getcwd() != '/content/tacotron2':
    os.chdir('tacotron2')
import time
import argparse
import math
from numpy import finfo

import torch
from distributed import apply_gradient_allreduce
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

from model import Tacotron2
from data_utils import TextMelLoader, TextMelCollate
from loss_function import Tacotron2Loss
from logger import Tacotron2Logger
from hparams import create_hparams
 
import random
import numpy as np

import layers
from utils import load_wav_to_torch, load_filepaths_and_text
from text import text_to_sequence
from math import e
#from tqdm import tqdm # Terminal
#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM
from tqdm.notebook import tqdm # Modern Notebook TQDM
from distutils.dir_util import copy_tree
import matplotlib.pylab as plt

"""# Parameters

"""

# ---- Replace .wav with .npy in filelists ----
!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt
# ---- Replace .wav with .npy in filelists ----

warm_start=False#sorry bout that
n_gpus=1
rank=0
group_name=None

# ---- DEFAULT PARAMETERS DEFINED HERE ----

hparams = create_hparams()
model_filename = 'current_model'
hparams.training_files = "filelists/train.txt"
hparams.validation_files = "filelists/val.txt"
#hparams.use_mmi=True,          # not used in this notebook
#hparams.use_gaf=True,          # not used in this notebook
#hparams.max_gaf=0.5,           # not used in this notebook
#hparams.drop_frame_rate = 0.2  # not used in this notebook
hparams.p_attention_dropout=0.1
hparams.p_decoder_dropout=0.1
hparams.decay_start = 15000
hparams.A_ = 5e-4
hparams.B_ = 8000
hparams.C_ = 0
hparams.min_learning_rate = 1e-5
generate_mels = True
hparams.show_alignments = True
alignment_graph_height = 600
alignment_graph_width = 1000
hparams.batch_size = 32
hparams.load_mel_from_disk = True
hparams.ignore_layers = []
hparams.epochs = 10000
torch.backends.cudnn.enabled = hparams.cudnn_enabled
torch.backends.cudnn.benchmark = hparams.cudnn_benchmark
output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints
log_directory = '/content/tacotron2/logs' # Location to save Log files locally
log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)
checkpoint_path = output_directory+(r'/')+model_filename

"""# Text to speech from given wav file """

!cd /content

"""**Downsampling**"""

path = "/content/sirca-kosk-sabahattin-ali-kisa-oyku.mp3"

y, s = librosa.load(path, sr=22050,mono=True) # Downsample 44.1kHz to 8kHz
sf.write('stereo_file.wav', y, 22050, 'PCM_16')

"""**Separating Chunks**"""

filename = "/content/wavs"

def silence_based_conversion(path = "sirca-kosk-sabahattin-ali-kisa-oyku.mp3.wav"):
    song = AudioSegment.from_file(path, sample_rate_Hz=22050,sample_width=2,channels=1)
    song.set_channels(1)
    train =  ""
    val = ""
    fh = open("recognized.txt", "w+")
    train_file = open("train.txt", "w+")
    val_file = open("val.txt", "w+")

    chunks = split_on_silence(song,
        min_silence_len = 500, #Split from silence 
        silence_thresh = -50,  #Min Silence thresh
    )
  
    # create a directory to store the audio chunks.
    try:
        os.mkdir('wavs')
    except(FileExistsError):
        pass
  
    os.chdir('wavs')
  
    i = 0

    for chunk in chunks:
        chunk_silent = AudioSegment.silent(duration = 1300) #Add duration the files 
        audio_chunk = chunk_silent + chunk + chunk_silent 
        print("saving chunk{0}.wav".format(i))

        audio_chunk.export("./chunk{0}.wav".format(i), bitrate ='16k',format ="wav")
        filename = 'chunk'+str(i)+'.wav' ##Create wav files
        print("Processing chunk "+str(i))
        file = filename

        #Recognize file 
        r = sr.Recognizer()
        with sr.AudioFile(file) as source:
            r.adjust_for_ambient_noise(source)
            audio_listened = r.listen(source)
  
        try:
            # try converting it to text
            rec = r.recognize_google(audio_listened, language = 'tr-TR')
            # write the output to the file.
            
            fh.write(rec+". ")

            #Create text file for every wav file and save it 
            with open("chunk{0}.txt".format(i), 'w') as f:
              f.write(rec)
              print("saving chunk{0}.txt".format(i))

            rand = random.randint(1, 100)

            if rand <= 93:  
              with open("/content/tacotron2/filelists/train.txt", 'w') as fx:
                train += str("/content/tacotron2/wavs/chunk{0}.wav".format(i)) + "|" + rec + "\n"
                fx.write(train)
                print("Train is saving chunk{0}.txt".format(i))  

            elif rand > 93:
              with open("/content/val.txt", 'w') as fx:
                  val += str("/content/tacotron2/wavs/chunk{0}.wav".format(i)) + "|" + rec + "\n"
                  fx.write(val)
                  print("Val is saving chunk{0}.txt".format(i))  

              
        # catch any errors. 
        except sr.UnknownValueError:
            print("Couldn't understand the audio")
            os.remove("/content/tacotron2/wavs/chunk{0}.wav".format(i))
  
        except sr.RequestError as e:
            print("Could not request results. check your internet connection")
        i += 1
  
    os.chdir('..')

path2 ="/content/tacotron2/stereo_file.wav"
silence_based_conversion(path2)

#os.replace("/content/train.txt", "/content/tacotron2/filelists/train.txt")
#os.replace("/content/val.txt", "/content/tacotron2/filelists/val.txt")

"""# Create Mels"""

def create_mels():
    print("Generating Mels")
    stft = layers.TacotronSTFT(
                hparams.filter_length, hparams.hop_length, hparams.win_length,
                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,
                hparams.mel_fmax)
    def save_mel(filename):
        audio, sampling_rate = load_wav_to_torch(filename)
        if sampling_rate != stft.sampling_rate:
            raise ValueError("{} {} SR doesn't match target {} SR".format(filename, 
                sampling_rate, stft.sampling_rate))
        audio_norm = audio / hparams.max_wav_value
        audio_norm = audio_norm.unsqueeze(0)
        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)
        melspec = stft.mel_spectrogram(audio_norm)
        melspec = torch.squeeze(melspec, 0).cpu().numpy()
        np.save(filename.replace('.wav', ''), melspec)

    import glob
    wavs = glob.glob('wavs/*.wav')
    for i in tqdm(wavs):
        save_mel(i)
create_mels()

"""# Delete files if required"""

# Commented out IPython magic to ensure Python compatibility.
# %#rm -rf /content/tacotron2/wavs ##Delete chunk files if you need to

"""

# Tacotron2"""

cd /content/tacotron2

!python train.py --output_directory=outdir --log_directory=logdir

!zip -r /content/file.zip /content/tacotron2/wavs
files.download("/content/file.zip")